{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4a6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c970b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=60),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.7, 1.3)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
    "    transforms.RandomResizedCrop((300, 300), scale=(0.7, 1.0), ratio=(0.75, 1.333)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize to ImageNet standards\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder('train', transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder('valid', transform=val_transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Classes\n",
    "class_names = train_dataset.classes\n",
    "print(class_names)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Documents\\plant_diseases\\myplantenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Asus\\Documents\\plant_diseases\\myplantenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained EfficientNet-B3\n",
    "model = models.efficientnet_b3(pretrained=True)\n",
    "\n",
    "# Freeze the feature extractor layers\n",
    "# We will unfreeze later for fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the final layer\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(128, len(class_names)) # Use len(class_names) for output classes\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Changes for Fine-tuning ---\n",
    "# Unfreeze some layers for fine-tuning\n",
    "# Here we unfreeze the last few blocks of the feature extractor\n",
    "# The exact layers to unfreeze might require experimentation\n",
    "# Example: Unfreeze the last two blocks (adapt based on model structure)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name or 'features.6' in name or 'features.7' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False # Ensure other layers remain frozen initially\n",
    "\n",
    "# Define optimizer with different learning rates for different parameter groups\n",
    "# Higher learning rate for the new classifier layers\n",
    "# Lower learning rate for the fine-tuned pre-trained layers\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.classifier.parameters(), 'lr': 0.001}, # Higher LR for new layers\n",
    "    {'params': [param for name, param in model.named_parameters() if 'classifier' not in name and param.requires_grad], 'lr': 0.0001} # Lower LR for fine-tuned layers\n",
    "], lr=0.0001) # Default LR if no group matches (though all should be in groups here)\n",
    "\n",
    "# Add a learning rate scheduler (optional but recommended for fine-tuning)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "# --- End Changes for Fine-tuning ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.5579, Train Acc: 0.8441, Val Loss: 0.2435, Val Acc: 0.9488\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 72\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Pass the scheduler to the training function\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Increased epochs as example\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, history_file)\u001b[0m\n\u001b[0;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 18\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     correct_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scheduler:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=10, history_file='history.json'):\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_patience = 5 # Added early stopping patience\n",
    "    patience_counter = 0 # Added patience counter\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, correct_train = 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            correct_train += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        val_loss, correct_val = 0, 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                correct_val += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        # Calculate accuracy and average loss\n",
    "        train_loss /= len(train_loader) # Calculate average train loss per batch\n",
    "        val_loss /= len(val_loader)   # Calculate average val loss per batch\n",
    "        train_acc = correct_train / len(train_loader.dataset)\n",
    "        val_acc = correct_val / len(val_loader.dataset)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # Optionally save the best model here\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break # Stop training loop\n",
    "\n",
    "    # Load the best model state if saved\n",
    "    if os.path.exists('best_model.pth'):\n",
    "        model.load_state_dict(torch.load('best_model.pth'))\n",
    "        print(\"Loaded best model state.\")\n",
    "\n",
    "    # Save final history to JSON file\n",
    "    with open(history_file, 'w') as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "\n",
    "    return history\n",
    "\n",
    "# Train the model\n",
    "# Pass the scheduler to the training function\n",
    "history = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=scheduler, num_epochs=10) # Increased epochs as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained model (or use the best_model.pth saved during early stopping)\n",
    "torch.save(model.state_dict(), 'efficientnet_b3_final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Evaluation after training (using the best model loaded)\n",
    "print(\"\\nEvaluation on Validation Set:\")\n",
    "model.eval()\n",
    "val_labels, val_preds = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        val_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(val_labels, val_preds))\n",
    "print(\"\\nClassification Report:\")\n",
    " \n",
    "# Ensure class_names is available and correct for target_names\n",
    "print(classification_report(val_labels, val_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need ROC AUC, ensure it's applicable (binary or appropriate multi-class handling)\n",
    "# Example for binary or macro/weighted average multi-class:\n",
    "# try:\n",
    "#     val_probs = []\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             val_probs.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "#     val_probs = np.array(val_probs)\n",
    "#     # For multi-class, you might need to calculate per-class AUC or macro/weighted average\n",
    "#     # Example: Macro average AUC for multi-class\n",
    "#     if len(class_names) > 2:\n",
    "#          roc_auc = roc_auc_score(val_labels, val_probs, multi_class='ovo', average='macro') # 'ovo' or 'ovr'\n",
    "#          print(f\"\\nMacro Average ROC AUC: {roc_auc:.4f}\")\n",
    "# except ValueError as e:\n",
    "#     print(f\"Could not calculate ROC AUC: {e}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred during ROC AUC calculation: {e}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myplantenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
